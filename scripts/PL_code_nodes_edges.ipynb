{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ampazio/hallow-universe/blob/main/Code_nodes_edges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pierwszy krok to analiza leksykalna w MAXQDA**.\n",
        "\n",
        "Najpierw otwieramy MAXDiction > Słownik. Tam dodajemy kategorie. Jedna kategoria to jeden kod (pamiętamy, żeby nie było spacji w kilkuczłonowych nazwach). Do każdej kategorii dodajemy pozycje, jakie słownik powinien wyszukiwać. Np. KATEGORIA: JonathanRoumie > pozycja: Jonathan Roumie.\n",
        "\n",
        "Potem przeprowadzamy analizę treści opartą na słowniku i kodujemy dokumenty wynikami."
      ],
      "metadata": {
        "id": "nitl4oGPM5xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Najpierw potrzebujemy dokonać ekstrakcji naszych kodów z Maxqda**. Pilnujemy, żeby każdy kod stanowił JEDNO słowo (brak spacji).\n",
        "\n",
        "Aktywujemy wszystkie dokumenty i wszystkie kody.\n",
        "\n",
        "Następnie idziemy do zakładki Analiza > Kompleksowe wyszukiwanie segmentów.\n",
        "W oknie, które się otwiera, wybieramy opcję \"Intersekcja (zestaw)\", zaznaczamy wszystkie kody i ustawiamy minimalną liczbę kodów na 2.\n",
        "\n",
        "Następnie w zakładce Wyszukane segmenty w dolnej części panelu klikamy \"Eksportuj jako plik Excel\". Wybieramy opcję \"inne kody przypisane do segmentu\". Zapisujemy plik na dysku.\n",
        "\n",
        "Otwieramy plik w Excelu i zapisujemy go jako .csv UTF-8\n",
        "\n",
        "Ten csv wgrywamy do COLABa i wstawiamy ścieżkę do niego do poniższego skryptu."
      ],
      "metadata": {
        "id": "1k1HLfsDLbB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## PRÓBA CZYSZCZENIA PLIKU WYEKSPORTOWANEGO Z MAXQDY\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Wczytaj plik CSV\n",
        "file_path = '/content/MAXQDA 2_KODY.csv'  # Zmień na ścieżkę do Twojego pliku\n",
        "data = pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
        "\n",
        "# Zmień nazwę kolumny B\n",
        "data.rename(columns={'Inne kody przypisane do segmentu': 'kod'}, inplace=True)\n",
        "\n",
        "# Oczyść dane w kolumnie 'kod'\n",
        "if 'kod' in data.columns:\n",
        "    data['kod'] = data['kod'].astype(str).str.replace(r'\\(Waga: 0\\)', '', regex=True)  # Usuń frazy \"(Waga: 0)\"\n",
        "    data['kod'] = data['kod'].str.replace('\"', '')  # Usuń zbędne cudzysłowy\n",
        "    data['kod'] = data['kod'].str.split().apply(lambda x: ';'.join(filter(None, x)))  # Połącz pozostałe słowa średnikami\n",
        "\n",
        "# Zapisz przetworzony plik\n",
        "output_path = 'przetworzony_plik.csv'  # Ścieżka do pliku wyjściowego\n",
        "data.to_csv(output_path, sep=';', index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Plik został przetworzony i zapisany jako {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-AK8h7n-nuK",
        "outputId": "c56840ff-2971-4349-b8b6-31be82a7d534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plik został przetworzony i zapisany jako przetworzony_plik.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mamy już wyczyszczony plik, który możemy wstawić do poniższego skryptu."
      ],
      "metadata": {
        "id": "SnqTl75iMRcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POPRAWNY Kod mapujący relacje między kodami dla każdego segmentu\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Załaduj dane, używając średnika jako separatora\n",
        "data = pd.read_csv('/content/przetworzony_plik.csv', delimiter=';')\n",
        "\n",
        "# Sprawdź nazwy kolumn\n",
        "print(data.columns)\n",
        "\n",
        "# Usuń nadmiarowe spacje w nazwach kolumn\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Sprawdź, czy 'kod' nie zawiera pustych wartości\n",
        "data['kod'] = data['kod'].fillna('')  # Zastępuje puste wartości pustymi stringami\n",
        "\n",
        "# Rozdziel kolumnę \"kod\" na osobne kody (teraz będą to listy ID)\n",
        "data['kod'] = data['kod'].str.split(';')\n",
        "\n",
        "# Stwórz listę wszystkich unikalnych kodów (aktorów z kolumny 'kod')\n",
        "all_codes = set()\n",
        "for codes in data['kod']:\n",
        "    all_codes.update(codes)\n",
        "\n",
        "# Przypisz unikalne numery ID do każdego kodu\n",
        "code_to_id = {code: idx + 1 for idx, code in enumerate(all_codes)}\n",
        "\n",
        "# Zamień kody na ich ID w kolumnie 'kod'\n",
        "data['kod'] = data['kod'].apply(lambda x: [code_to_id[code] for code in x])\n",
        "\n",
        "# Tworzymy listę krawędzi (relacje między kodami w ramach tego samego segmentu)\n",
        "edges = []\n",
        "\n",
        "# Dla każdego wiersza w danych\n",
        "for idx, row in data.iterrows():\n",
        "    codes = row['kod']  # Lista kodów przypisanych do segmentu\n",
        "\n",
        "    # Tworzymy wszystkie unikalne pary kodów w ramach jednego segmentu\n",
        "    for i in range(len(codes)):\n",
        "        for j in range(i + 1, len(codes)):\n",
        "            edges.append([codes[i], codes[j], 'Undirected', 1])\n",
        "\n",
        "# Tworzymy DataFrame dla krawędzi\n",
        "edges_df = pd.DataFrame(edges, columns=['Source', 'Target', 'Type', 'Weight'])\n",
        "\n",
        "# Tworzymy DataFrame dla węzłów (code_id i code_name)\n",
        "nodes = pd.DataFrame({\n",
        "    'Id': [code_to_id[code] for code in all_codes],\n",
        "    'Label': [code for code in all_codes]\n",
        "})\n",
        "\n",
        "# Zapisz pliki CSV\n",
        "nodes.to_csv('nodes.csv', index=False, sep=';')  # Węzły (kody)\n",
        "edges_df.to_csv('edges.csv', index=False, sep=';')  # Krawędzie (relacje między kodami)\n",
        "\n",
        "print(\"Pliki nodes.csv i edges.csv zostały wygenerowane.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEV-BaeZHhNX",
        "outputId": "411582af-1e4d-4dde-961a-9afabf8be0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Segment', 'kod'], dtype='object')\n",
            "Pliki nodes.csv i edges.csv zostały wygenerowane.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrzucamy oba pliki do Gephi.\n",
        "Zmieniamy układ m.in. przy użyciu algorytmu Fruchterman Reingold. Ustawiamy wielkość nodes, tak żeby były zależne od stopnia usieciowienia. Wyodrębniamy communities przy użyciu statystyki Modularity itp."
      ],
      "metadata": {
        "id": "m120fsloG0mx"
      }
    }
  ]
}
